{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c342bf",
   "metadata": {},
   "source": [
    "# Zadanie 3 – Rozpoznawanie intencji użytkownika\n",
    "Celem jest zbudowanie prostego modelu NLP, który będzie klasyfikował wypowiedzi użytkowników według ich intencji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2999112",
   "metadata": {},
   "source": [
    "## Dane wejściowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a51be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        'Chciałbym zamówić pizzę',\n",
    "        'Pokaż mi pogodę w Warszawie',\n",
    "        'Potrzebuję pomocy z logowaniem',\n",
    "        'Czy mogę zmienić termin wizyty?',\n",
    "        'Jakie są godziny otwarcia?',\n",
    "        'Zamów mi kawę na wynos',\n",
    "        'Nie mogę się zalogować do systemu',\n",
    "        'Jaka jest temperatura w Krakowie?',\n",
    "        'Proszę o nową fakturę',\n",
    "        'Chcę zamówić sushi'\n",
    "    ],\n",
    "    'intent': [\n",
    "        'zamówienie_jedzenia',\n",
    "        'sprawdzenie_pogody',\n",
    "        'wsparcie_logowanie',\n",
    "        'zmiana_terminu',\n",
    "        'informacja_godziny',\n",
    "        'zamówienie_jedzenia',\n",
    "        'wsparcie_logowanie',\n",
    "        'sprawdzenie_pogody',\n",
    "        'fakturowanie',\n",
    "        'zamówienie_jedzenia'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bbee82",
   "metadata": {},
   "source": [
    "## Tokenizacja i kodowanie etykiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "padded = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(df['intent'].unique())}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "labels = df['intent'].map(label2id).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08266a5",
   "metadata": {},
   "source": [
    "## Budowa i trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a25097",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=1000, output_dim=16, input_length=10),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label2id), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c4a60",
   "metadata": {},
   "source": [
    "## Predykcja nowych wypowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e38817",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Zamów makaron na wieczór',\n",
    "    'Proszę o zmianę godziny wizyty',\n",
    "    'Jaka pogoda będzie jutro?',\n",
    "    'Nie mogę odzyskać hasła',\n",
    "    'Chciałbym zamówić lody'\n",
    "]\n",
    "seq = tokenizer.texts_to_sequences(test_sentences)\n",
    "pad = pad_sequences(seq, maxlen=10, padding='post')\n",
    "preds = model.predict(pad)\n",
    "\n",
    "for i, p in enumerate(preds):\n",
    "    print(f'{test_sentences[i]} -> {id2label[np.argmax(p)]}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
