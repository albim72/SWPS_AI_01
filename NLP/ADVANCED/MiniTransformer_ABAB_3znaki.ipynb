{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab70d50b",
      "metadata": {
        "id": "ab70d50b"
      },
      "source": [
        "# MiniTransformer Demo – Praktyczne użycie\n",
        "\n",
        "Ten notebook pokazuje działający przykład MiniTransformera w PyTorch, który uczy się rozpoznawać wzorzec sekwencyjny (np. ABABAB...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ff028c81",
      "metadata": {
        "id": "ff028c81"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e309a826",
      "metadata": {
        "id": "e309a826"
      },
      "outputs": [],
      "source": [
        "# Słownik i kodowanie\n",
        "vocab = ['A', 'B','F']\n",
        "stoi = {s: i for i, s in enumerate(vocab)}  # string to index\n",
        "itos = {i: s for s, i in stoi.items()}      # index to string\n",
        "\n",
        "# Generujemy dane: np. \"ABABABAB...\"\n",
        "data = [stoi[c] for c in \"ABABABFABABABFABABABFAB\"]\n",
        "block_size = 6\n",
        "\n",
        "# Tworzymy zbiór treningowy\n",
        "sequences, targets = [], []\n",
        "for i in range(len(data) - block_size):\n",
        "    seq = data[i:i+block_size]\n",
        "    target = data[i+1:i+block_size+1]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "x_train = torch.tensor(sequences)\n",
        "y_train = torch.tensor(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "191ec339",
      "metadata": {
        "id": "191ec339"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "        scores = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
        "        mask = torch.tril(torch.ones(T, T)).to(x.device)\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        att = F.softmax(scores, dim=-1)\n",
        "        out = att @ v\n",
        "        return self.ln(self.proj(out) + x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a5b5e73",
      "metadata": {
        "id": "2a5b5e73"
      },
      "outputs": [],
      "source": [
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, block_size):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(1024, embed_dim)\n",
        "        self.transformer = TransformerBlock(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_emb(idx)\n",
        "        position_embeddings = self.pos_emb(torch.arange(T).to(idx.device))\n",
        "        x = token_embeddings + position_embeddings\n",
        "        x = self.transformer(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1c4e71b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c4e71b9",
        "outputId": "bded33cb-9fa2-4392-c578-1fba4504e421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoka 0, Strata: 1.0327\n",
            "Epoka 40, Strata: 0.1631\n",
            "Epoka 80, Strata: 0.1596\n",
            "Epoka 120, Strata: 0.1593\n",
            "Epoka 160, Strata: 0.1592\n",
            "Epoka 200, Strata: 0.1601\n",
            "Epoka 240, Strata: 0.1591\n",
            "Epoka 280, Strata: 0.1591\n"
          ]
        }
      ],
      "source": [
        "model = MiniTransformer(vocab_size=3, embed_dim=16, block_size=5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(300):\n",
        "    logits = model(x_train)\n",
        "    B, T, C = logits.shape\n",
        "    loss = loss_fn(logits.view(B*T, C), y_train.view(B*T))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 40 == 0:\n",
        "        print(f\"Epoka {epoch}, Strata: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e5aa25a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5aa25a5",
        "outputId": "53f08939-9683-41ab-8aec-40e44988d2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wygenerowana sekwencja: FABBABABFABABABFABABABFABABAB\n"
          ]
        }
      ],
      "source": [
        "def generate(model, start, steps=8):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([[stoi[s] for s in start]], dtype=torch.long)\n",
        "    for _ in range(steps):\n",
        "        logits = model(idx[:, -block_size:])\n",
        "        last = logits[:, -1, :]\n",
        "        probs = F.softmax(last, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "    return ''.join([itos[i.item()] for i in idx[0]])\n",
        "\n",
        "# Przykład użycia\n",
        "print(\"Wygenerowana sekwencja:\", generate(model, start=\"FABB\", steps=25))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}