{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cpdv7zVY2wz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini Transformer - BUMBLEBEE - CEL: zbudować mały model jezykowy na self-attension z jedną warstwą transformera, która potrafi uczyć się prostych zależnosci w sekwencjach (przwidywanie następnego tokenu)"
      ],
      "metadata": {
        "id": "pSswmQvQfNA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "eclEMopZfqNy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KROK2: definiujemy poejdynczy blok transformera"
      ],
      "metadata": {
        "id": "T5n1LVLxf2VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.key = nn.Linear(embed_dim, embed_dim,bias=False)\n",
        "        self.query = nn.Linear(embed_dim, embed_dim,bias=False)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim,bias=False)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        #KROK3: mechanizm self-attention\n",
        "\n",
        "        def forward(self, x):\n",
        "            B, T, C = x.shape #B=batch, T=sequence length, C=embedding size\n",
        "            k = self.key(x)\n",
        "            q = self.query(x)\n",
        "            v = self.value(x)\n",
        "\n",
        "            scores = q @ k.transpose(-2, -1) / (C**0.5) #B,T,T -> oblicza podobieństwo między tokenami!\n",
        "            mask = torch.tril(torch.ones(T,T)).to(x.device) #nie patrzymy na przyszłe tokeny - (ważne w modelach generatywnych)\n",
        "            scores = scores.masked_fill(mask == 0,float('-inf')) #maskowanie przyszłości\n",
        "\n",
        "            att = F.softmax(scores, dim=-1) #rozkład uwagi\n",
        "            out=att@v #mieszanie wartości zgodnie z uwagą , v to \"treść\" tokenu który bierzemy\n",
        "            return self.ln(self.proj(out) + x) #resztkowe połączenie i normalizacja\n",
        "\n"
      ],
      "metadata": {
        "id": "xp0QmM8bf8nM"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}