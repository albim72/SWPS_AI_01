{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab70d50b",
   "metadata": {},
   "source": [
    "#MiniTransformer Demo – Praktyczne użycie\n",
    "\n",
    "Ten notebook pokazuje działający przykład MiniTransformera w PyTorch, który uczy się rozpoznawać wzorzec sekwencyjny (np. ABABAB...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff028c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Słownik i kodowanie\n",
    "vocab = ['A', 'B']\n",
    "stoi = {s: i for i, s in enumerate(vocab)}  # string to index\n",
    "itos = {i: s for s, i in stoi.items()}      # index to string\n",
    "\n",
    "# Generujemy dane: np. \"ABABABAB...\"\n",
    "data = [stoi[c] for c in \"ABABABABABABABABABAB\"]\n",
    "block_size = 5\n",
    "\n",
    "# Tworzymy zbiór treningowy\n",
    "sequences, targets = [], []\n",
    "for i in range(len(data) - block_size):\n",
    "    seq = data[i:i+block_size]\n",
    "    target = data[i+1:i+block_size+1]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "x_train = torch.tensor(sequences)\n",
    "y_train = torch.tensor(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ec339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        scores = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        mask = torch.tril(torch.ones(T, T)).to(x.device)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        att = F.softmax(scores, dim=-1)\n",
    "        out = att @ v\n",
    "        return self.ln(self.proj(out) + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_emb(idx)\n",
    "        position_embeddings = self.pos_emb(torch.arange(T).to(idx.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.transformer(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniTransformer(vocab_size=2, embed_dim=16, block_size=5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    logits = model(x_train)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, C), y_train.view(B*T))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 40 == 0:\n",
    "        print(f\"Epoka {epoch}, Strata: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start, steps=8):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[s] for s in start]], dtype=torch.long)\n",
    "    for _ in range(steps):\n",
    "        logits = model(idx[:, -block_size:])\n",
    "        last = logits[:, -1, :]\n",
    "        probs = F.softmax(last, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return ''.join([itos[i.item()] for i in idx[0]])\n",
    "\n",
    "# Przykład użycia\n",
    "print(\"Wygenerowana sekwencja:\", generate(model, start=\"ABABA\", steps=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja macierzy uwagi (attention weights)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_attention_weights(model, input_sequence):\n",
    "    with torch.no_grad():\n",
    "        idx = torch.tensor([[stoi[s] for s in input_sequence]], dtype=torch.long)\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = model.token_emb(idx)\n",
    "        position_embeddings = model.pos_emb(torch.arange(T).to(idx.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # ręcznie przechodzimy przez TransformerBlock\n",
    "        q = model.transformer.query(x)\n",
    "        k = model.transformer.key(x)\n",
    "        att_scores = q @ k.transpose(-2, -1) / (q.shape[-1] ** 0.5)\n",
    "        mask = torch.tril(torch.ones(T, T)).to(x.device)\n",
    "        att_scores = att_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        att_weights = F.softmax(att_scores, dim=-1)\n",
    "        return att_weights.squeeze().cpu()\n",
    "\n",
    "# Przykład\n",
    "sequence = \"ABABA\"\n",
    "weights = get_attention_weights(model, sequence)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights, cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Attention weights dla sekwencji: \" + sequence)\n",
    "plt.xlabel(\"Tokeny na które patrzy\")\n",
    "plt.ylabel(\"Token patrzący\")\n",
    "plt.xticks(range(len(sequence)), list(sequence))\n",
    "plt.yticks(range(len(sequence)), list(sequence))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
